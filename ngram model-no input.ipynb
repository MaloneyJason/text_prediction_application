{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "from collections import defaultdict, Counter # for the model\n",
    "from nltk.util import ngrams \n",
    "import pandas as pd # dataframes \n",
    "import numpy as np \n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import glob # read multiple files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "12500\n",
      "12500\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "train_pos_filenames = glob.glob('/Users/jasonmaloney/Documents/Syracuse/IST 736 Text Mining/Text Mining Project/aclImdb/train/pos/*.txt')\n",
    "train_neg_filenames = glob.glob('/Users/jasonmaloney/Documents/Syracuse/IST 736 Text Mining/Text Mining Project/aclImdb/train/neg/*.txt')\n",
    "test_pos_filenames = glob.glob('/Users/jasonmaloney/Documents/Syracuse/IST 736 Text Mining/Text Mining Project/aclImdb/test/pos/*.txt')\n",
    "test_neg_filenames = glob.glob('/Users/jasonmaloney/Documents/Syracuse/IST 736 Text Mining/Text Mining Project/aclImdb/test/neg/*.txt')\n",
    "print(len(train_pos_filenames))\n",
    "print(len(train_neg_filenames))\n",
    "print(len(test_pos_filenames))\n",
    "print(len(test_neg_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.01 µs\n",
      "train_pos_text:\n",
      "for a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. imagine a movie where joe piscopo is actually funny! maureen stapleton is a scene stealer. the moroni character is an absolute scream. watch for alan \"the skipper\" hale jr. as a police sgt.\n",
      "\n",
      "train_neg_text:\n",
      "working with one of the best shakespeare sources, this film manages to be creditable to it's source, whilst still appealing to a wider audience.branagh steals the film from under fishburne's nose, and there's a talented cast on good form.\n",
      "\n",
      "test_pos_text:\n",
      "based on an actual story, john boorman shows the struggle of an american doctor, whose husband and son were murdered and she was continually plagued with her loss. a holiday to burma with her sister seemed like a good idea to get away from it all, but when her passport was stolen in rangoon, she could not leave the country with her sister, and was forced to stay back until she could get i.d. papers from the american embassy. to fill in a day before she could fly out, she took a trip into the countryside with a tour guide. \"i tried finding something in those stone statues, but nothing stirred in me. i was stone myself.\" suddenly all hell broke loose and she was caught in a political revolt. just when it looked like she had escaped and safely boarded a train, she saw her tour guide get beaten and shot. in a split second she decided to jump from the moving train and try to rescue him, with no thought of herself. continually her life was in danger. here is a woman who demonstrated spontaneous, selfless charity, risking her life to save another. patricia arquette is beautiful, and not just to look at; she has a beautiful heart. this is an unforgettable story. \"we are taught that suffering is the one promise that life always keeps.\"\n",
      "\n",
      "test_neg_text:\n",
      "alan rickman & emma thompson give good performances with southern/new orleans accents in this detective flick. it's worth seeing for their scenes- and rickman's scene with hal holbrook. these three actors mannage to entertain us no matter what the movie, it seems. the plot for the movie shows potential, but one gets the impression in watching the film that it was not pulled off as well as it could have been. the fact that it is cluttered by a rather uninteresting subplot and mostly uninteresting kidnappers really muddles things. the movie is worth a view- if for nothing more than entertaining performances by rickman, thompson, and holbrook.\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# read the contents of the train_pos files into a list (each list element is one review)\n",
    "# clean line breaks and html tags like <br\\>\n",
    "train_pos_text = []\n",
    "for filename in train_pos_filenames:\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\n|<\\w+\\s/>', '', text)\n",
    "        train_pos_text.append(text)\n",
    "print(\"train_pos_text:\")\n",
    "print(train_pos_text[0])\n",
    "\n",
    "\n",
    "# read the contents of the train_pos files into a list (each list element is one review)\n",
    "train_neg_text = []\n",
    "for filename in train_neg_filenames:\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\n|<\\w+\\s/>', '', text)\n",
    "        train_neg_text.append(text)\n",
    "print(\"\\ntrain_neg_text:\")\n",
    "print(train_neg_text[0])\n",
    "\n",
    "test_pos_text = []\n",
    "for filename in test_pos_filenames:\n",
    "    with open(filename, encoding = 'utf-8') as f:\n",
    "        text = f.read()\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\n|<\\w+\\s/>', '', text)\n",
    "        test_pos_text.append(text)\n",
    "print('\\ntest_pos_text:')\n",
    "print(test_pos_text[0])\n",
    "\n",
    "test_neg_text = []\n",
    "for filename in test_neg_filenames:\n",
    "    with open(filename, encoding = 'utf-8') as f:\n",
    "        text = f.read()\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\n|<\\w+\\s/>', '', text)\n",
    "        test_neg_text.append(text)\n",
    "print('\\ntest_neg_text:')\n",
    "print(test_neg_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11772360 tokens. \n",
      "There are 104138 unique tokens. \n"
     ]
    }
   ],
   "source": [
    "# combine all reviews into a long string to get more n-grams\n",
    "words = ''\n",
    "for rev in train_pos_text:\n",
    "    words += ' ' + rev\n",
    "\n",
    "for rev in train_neg_text:\n",
    "    words += ' ' + rev\n",
    "    \n",
    "for rev in test_pos_text:\n",
    "    words += ' ' + rev\n",
    "\n",
    "for rev in test_neg_text:\n",
    "    words += ' ' + rev\n",
    "\n",
    "# define tokenizer to get words\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# words has all the review as one string\n",
    "tokens = tokenizer.tokenize(words)\n",
    "print('There are {} tokens. '.format(len(tokens)))\n",
    "print('There are {} unique tokens. '.format(len(set(tokens))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the default dictionary - used for saving the model with dill library\n",
    "def default_dict():\n",
    "    return defaultdict(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the N-gram Models\n",
    "N = 2, 3, 4, 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the conditional probability of word2 given word1\n",
    "# P(word2|word1)\n",
    "def build_bigram_model():\n",
    "    bigram_model = defaultdict(default_dict) # create a model mold\n",
    "    # collect all bigrams for (w1, w2)\n",
    "    for word1, word2 in ngrams(tokens, 2):\n",
    "        # increase the count (frequency of tokens)\n",
    "        bigram_model[word1][word2] += 1\n",
    "    # compute the probability P(word2|word1)\n",
    "    for word1 in bigram_model:\n",
    "        # get total count of bigrams with word1\n",
    "        total_count = float(sum(bigram_model[word1].values()))\n",
    "        for word2 in bigram_model[word1]:\n",
    "            # number of bigrams (word1 word2)/total\n",
    "            bigram_model[word1][word2] /= total_count\n",
    "    return bigram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "bigram_model = build_bigram_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict the next word based on bigram model\n",
    "def bigram_predict_next_word(first_word):\n",
    "    if len(bigram_model[first_word]) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        # tokenize user input\n",
    "        user_tokens = tokenizer.tokenize(first_word)\n",
    "        #print('input:', user_tokens[0])\n",
    "        # get probabilities of next word given user input\n",
    "        prob_list = bigram_model[user_tokens[0]].values()\n",
    "        #print(prob_list)\n",
    "        # find the max prob\n",
    "        most_likely = max(prob_list)\n",
    "        #print(most_likely)\n",
    "        # predicted words\n",
    "        pred_words = [word for word, prob in bigram_model[user_tokens[0]].items() if prob == most_likely]\n",
    "    return pred_words[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: brad\n",
      "0.4711864406779661\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pitt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_predict_next_word('brad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trigram_model():\n",
    "    trigram_model = defaultdict(default_dict) # create a model mold\n",
    "    # collect trigrams for word1, word2, word3\n",
    "    for word1, word2, word3 in ngrams(tokens , 3):\n",
    "        # increase the count \n",
    "        trigram_model[word1, word2][word3] += 1\n",
    "        # compute the probability P(word1, word|word3)\n",
    "    for word1_word2 in trigram_model:\n",
    "        # get total count of trigrams with word1 word2\n",
    "        total_count = float(sum(trigram_model[word1_word2].values()))\n",
    "        for word3 in trigram_model[word1_word2]:\n",
    "            # number of trigrams/total\n",
    "            trigram_model[word1_word2][word3] /= total_count\n",
    "    return trigram_model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "trigram_model = build_trigram_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict next word with trigram model\n",
    "def trigram_predict_next_word(two_words):\n",
    "    # tokenize user input\n",
    "    user_tokens = tokenizer.tokenize(two_words)\n",
    "    #print(user_tokens)\n",
    "    if len(trigram_model[user_tokens[0], user_tokens[1]]) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        # get probabilities of next word\n",
    "        prob_list = trigram_model[user_tokens[0], user_tokens[1]].values()\n",
    "        # find the max prob\n",
    "        #print(prob_list)\n",
    "        most_likely = max(prob_list)\n",
    "        #print(most_likely)\n",
    "        # predicted words\n",
    "        pred_words = [word for word, prob in trigram_model[user_tokens[0], user_tokens[1]].items() if prob == most_likely]\n",
    "    return pred_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'that']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_predict_next_word('funny movie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fourgram_model():\n",
    "    fourgram_model = defaultdict(default_dict) # create a model mold\n",
    "    # collect 4-grams for word1, word2, word3, word4\n",
    "    for word1, word2, word3, word4 in ngrams(tokens, 4):\n",
    "        # increase the count\n",
    "        fourgram_model[word1, word2, word3][word4] += 1\n",
    "        # compute the probability P(word1, word2, word3|word4)\n",
    "    for word1_word2_word3 in fourgram_model:\n",
    "        # get total count of 4grams with word1, word2, word3\n",
    "        total_count = float(sum(fourgram_model[word1_word2_word3].values()))\n",
    "        for word4 in fourgram_model[word1_word2_word3]:\n",
    "            # number of 4grams/total\n",
    "            fourgram_model[word1_word2_word3][word4] /= total_count\n",
    "    return fourgram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourgram_model = build_fourgram_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourgram_predict_next_word(three_words):\n",
    "    # tokenize user input\n",
    "    user_tokens = tokenizer.tokenize(three_words)\n",
    "    if len(fourgram_model[user_tokens[0], user_tokens[1], user_tokens[2]]) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        # get probabilities of next word\n",
    "        prob_list = fourgram_model[user_tokens[0], user_tokens[1], user_tokens[2]].values()\n",
    "        # find max prob\n",
    "        most_likely = max(prob_list)\n",
    "        # get the predicted word(s)\n",
    "        pred_words = [word for word, prob in fourgram_model[user_tokens[0], user_tokens[1], user_tokens[2]].items() if prob == most_likely]\n",
    "    return pred_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourgram_predict_next_word('funny movie that')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fourgram_model():\n",
    "    fourgram_model = defaultdict(default_dict) # create a model mold\n",
    "    # collect 4-grams for word1, word2, word3, word4\n",
    "    for word1, word2, word3, word4 in ngrams(tokens, 4):\n",
    "        # increase the count\n",
    "        fourgram_model[word1, word2, word3][word4] += 1\n",
    "    # compute the probability P(word1, word2, word3|word4)\n",
    "    for word1_word2_word3 in fourgram_model:\n",
    "        # get total count of 4grams with word1, word2, word3\n",
    "        total_count = float(sum(fourgram_model[word1_word2_word3].values()))\n",
    "        for word4 in fourgram_model[word1_word2_word3]:\n",
    "            # number of 4grams/total\n",
    "            fourgram_model[word1_word2_word3][word4] /= total_count\n",
    "    return fourgram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build 5-gram model\n",
    "def build_fivegram_model():\n",
    "    fivegram_model = defaultdict(default_dict)\n",
    "    # collect 5-grams\n",
    "    for word1, word2, word3, word4, word5, in ngrams(tokens, 5):\n",
    "        fivegram_model[word1, word2, word3, word4][word5] += 1\n",
    "    # compute the probability P(word1, word2, word3, word4|word5)\n",
    "    for word1_word2_word3_word4 in fivegram_model:\n",
    "        # get total count of 5-grams\n",
    "        total_count = float(sum(fivegram_model[word1_word2_word3_word4].values()))\n",
    "        for word5 in fivegram_model[word1_word2_word3_word4]:\n",
    "            # number of 5grams/total\n",
    "            fivegram_model[word1_word2_word3_word4][word5] /= total_count\n",
    "    return fivegram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "fivegram_model = build_fivegram_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict next word with 5-gram model\n",
    "def fivegram_predict_next_word(four_words):\n",
    "    # tokenize user input\n",
    "    user_tokens = tokenizer.tokenize(four_words)\n",
    "    if len(fivegram_model[user_tokens[0], user_tokens[1], user_tokens[2], user_tokens[3]]) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        # get probabilities of next word\n",
    "        prob_list = fivegram_model[user_tokens[0], user_tokens[1], user_tokens[2], user_tokens[3]].values()\n",
    "        # find max prob\n",
    "        most_likely = max(prob_list)\n",
    "        # predicted words\n",
    "        pred_words = [word for word, prob in fivegram_model[user_tokens[0], user_tokens[1], user_tokens[2], user_tokens[3]].items() if prob == most_likely]\n",
    "    return pred_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cycle through the models to help return a predicted word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the length condition\n",
    "def ngram_prediction(words):\n",
    "    # tokenize the words\n",
    "    user_tokens = tokenizer.tokenize(words)\n",
    "    if len(user_tokens) >= 4:\n",
    "        pred_word = fivegram_predict_next_word(words)\n",
    "    elif len(user_tokens) == 3:\n",
    "        pred_word = fourgram_predict_next_word(words)\n",
    "    elif len(user_tokens) == 2:\n",
    "        pred_word = trigram_predict_next_word(words)\n",
    "    elif len(user_tokens) == 1:\n",
    "        pred_word = bigram_predict_next_word(words) \n",
    "    return pred_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['was']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test it out - uses only the first 4 words, not the last ones\n",
    "ngram_prediction('but when her passport was stolen in rangoon')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['struggle']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fivegram_predict_next_word('john boorman shows the')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
