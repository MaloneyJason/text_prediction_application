{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "from collections import defaultdict # for the model\n",
    "from nltk.util import ngrams\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "12500\n",
      "12500\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "# you need your machine's path \n",
    "# use * for the folder that has all the .txt files\n",
    "\n",
    "train_pos_filenames = glob.glob('/Users/jasonmaloney/Documents/Syracuse/IST 736 Text Mining/Text Mining Project/aclImdb/train/pos/*.txt')\n",
    "train_neg_filenames = glob.glob('/Users/jasonmaloney/Documents/Syracuse/IST 736 Text Mining/Text Mining Project/aclImdb/train/neg/*.txt')\n",
    "test_pos_filenames = glob.glob('/Users/jasonmaloney/Documents/Syracuse/IST 736 Text Mining/Text Mining Project/aclImdb/test/pos/*.txt')\n",
    "test_neg_filenames = glob.glob('/Users/jasonmaloney/Documents/Syracuse/IST 736 Text Mining/Text Mining Project/aclImdb/test/neg/*.txt')\n",
    "print(len(train_pos_filenames))\n",
    "print(len(train_neg_filenames))\n",
    "print(len(test_pos_filenames))\n",
    "print(len(test_neg_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_pos_text:\n",
      "for a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. imagine a movie where joe piscopo is actually funny! maureen stapleton is a scene stealer. the moroni character is an absolute scream. watch for alan \"the skipper\" hale jr. as a police sgt.\n",
      "train_neg_text:\n",
      "working with one of the best shakespeare sources, this film manages to be creditable to it's source, whilst still appealing to a wider audience.branagh steals the film from under fishburne's nose, and there's a talented cast on good form.\n",
      "test_pos_text:\n",
      "based on an actual story, john boorman shows the struggle of an american doctor, whose husband and son were murdered and she was continually plagued with her loss. a holiday to burma with her sister seemed like a good idea to get away from it all, but when her passport was stolen in rangoon, she could not leave the country with her sister, and was forced to stay back until she could get i.d. papers from the american embassy. to fill in a day before she could fly out, she took a trip into the countryside with a tour guide. \"i tried finding something in those stone statues, but nothing stirred in me. i was stone myself.\" suddenly all hell broke loose and she was caught in a political revolt. just when it looked like she had escaped and safely boarded a train, she saw her tour guide get beaten and shot. in a split second she decided to jump from the moving train and try to rescue him, with no thought of herself. continually her life was in danger. here is a woman who demonstrated spontaneous, selfless charity, risking her life to save another. patricia arquette is beautiful, and not just to look at; she has a beautiful heart. this is an unforgettable story. \"we are taught that suffering is the one promise that life always keeps.\"\n",
      "test_neg_text:\n",
      "alan rickman & emma thompson give good performances with southern/new orleans accents in this detective flick. it's worth seeing for their scenes- and rickman's scene with hal holbrook. these three actors mannage to entertain us no matter what the movie, it seems. the plot for the movie shows potential, but one gets the impression in watching the film that it was not pulled off as well as it could have been. the fact that it is cluttered by a rather uninteresting subplot and mostly uninteresting kidnappers really muddles things. the movie is worth a view- if for nothing more than entertaining performances by rickman, thompson, and holbrook.\n"
     ]
    }
   ],
   "source": [
    "# open files\n",
    "train_pos_text = []\n",
    "for filename in train_pos_filenames[:1000]:\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\n|<\\w+\\s/>', '', text)\n",
    "        train_pos_text.append(text)\n",
    "print(\"train_pos_text:\")\n",
    "print(train_pos_text[0])\n",
    "\n",
    "\n",
    "train_neg_text = []\n",
    "for filename in train_neg_filenames[:1000]:\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\n|<\\w+\\s/>', '', text)\n",
    "        train_neg_text.append(text)\n",
    "print(\"train_neg_text:\")\n",
    "print(train_neg_text[0])\n",
    "\n",
    "test_pos_text = []\n",
    "for filename in test_pos_filenames[:1000]:\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\n|<\\w+\\s/>', '', text)\n",
    "        test_pos_text.append(text)\n",
    "print(\"test_pos_text:\")\n",
    "print(test_pos_text[0])\n",
    "\n",
    "test_neg_text = []\n",
    "for filename in test_neg_filenames[:1000]:\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\n|<\\w+\\s/>', '', text)\n",
    "        test_neg_text.append(text)\n",
    "print(\"test_neg_text:\")\n",
    "print(test_neg_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get as a big long string\n",
    "words = ''\n",
    "for rev in train_pos_text:\n",
    "    words += ' ' + rev\n",
    "\n",
    "for rev in train_neg_text:\n",
    "    words += ' ' + rev\n",
    "    \n",
    "for rev in test_pos_text:\n",
    "    words += ' ' + rev\n",
    "\n",
    "for rev in test_neg_text:\n",
    "    words += ' ' + rev\n",
    "\n",
    "# define tokenizer to get words\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# words has all the review as one string\n",
    "tokens = tokenizer.tokenize(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of dictionaries\n",
    "# the inner function will take on counts (frequencies) \n",
    "# for probabilities\n",
    "bigram_model = defaultdict(lambda: defaultdict(lambda: 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill up the dictionary\n",
    "for word1, word2 in ngrams(tokens, 2):\n",
    "     # increase the count (frequency of tokens)\n",
    "    bigram_model[word1][word2] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the counts into probabilities\n",
    "for word1 in bigram_model:\n",
    "    #print(word1)\n",
    "    # sum freq of all the first 3 word combos of each phrase\n",
    "    total_count = float(sum(bigram_model[word1].values()))\n",
    "    # get each fourth word and divide each frequency by total count\n",
    "    # for conditional probabilities\n",
    "    for word2 in bigram_model[word1]:\n",
    "        # get the probability\n",
    "        # four_gram_model[word1_word2_word3]/total_count\n",
    "        bigram_model[word1][word2] /= total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return key for any value \n",
    "def getKeysByValue(dictOfElements, valueToFind):\n",
    "    listOfKeys = list()\n",
    "    listOfItems = dictOfElements.items()\n",
    "    for item  in listOfItems:\n",
    "        if item[1] == valueToFind:\n",
    "            listOfKeys.append(item[0])\n",
    "    return  listOfKeys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a word: i\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['was']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_text = input('Please enter a word: ')\n",
    "user_tokens = tokenizer.tokenize(user_text)\n",
    "prob_list = bigram_model.get(user_tokens[0]).values()\n",
    "most_likely = max(prob_list)\n",
    "getKeysByValue(dict(bigram_model[user_tokens[0]]), most_likely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02272288877743914"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get highest probability of the next word prediction\n",
    "max(bigram_model.get('a').values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of dictionaries\n",
    "# the inner function will take on counts (frequencies) \n",
    "# for probabilities\n",
    "trigram_model = defaultdict(lambda: defaultdict(lambda: 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill up the dictionary\n",
    "for word1, word2, word3 in ngrams(tokens, 3):\n",
    "     # increase the count (frequency of tokens)\n",
    "    trigram_model[word1, word2][word3] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the counts into probabilities\n",
    "for word1_word2 in trigram_model:\n",
    "    #print(word1_word2_word3)\n",
    "    # sum freq of all the first 3 word combos of each phrase\n",
    "    total_count = float(sum(trigram_model[word1_word2].values()))\n",
    "    # get each third word and divide each frequency by total count\n",
    "    # for conditional probabilities\n",
    "    for word3 in trigram_model[word1_word2]:\n",
    "        # get the probability\n",
    "        # four_gram_model[word1_word2_word3]/total_count\n",
    "        trigram_model[word1_word2][word3] /= total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter two words: robert deniro\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['and']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_text = input('Please enter two words: ')\n",
    "user_tokens = tokenizer.tokenize(user_text)\n",
    "#print (user_tokens)\n",
    "prob_list = trigram_model[user_tokens[0], user_tokens[1]].values()\n",
    "most_likely = max(prob_list)\n",
    "getKeysByValue(dict(trigram_model[user_tokens[0], user_tokens[1]]), most_likely)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-gram model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of dictionaries\n",
    "# the inner function will take on counts (frequencies) \n",
    "# for probabilities\n",
    "four_gram_model = defaultdict(lambda: defaultdict(lambda: 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill up the dictionary\n",
    "for word1, word2, word3, word4 in ngrams(tokens, 4):\n",
    "     # increase the count (frequency of tokens)\n",
    "    four_gram_model[word1, word2, word3][word4] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the counts into probabilities\n",
    "for word1_word2_word3 in four_gram_model:\n",
    "    #print(word1_word2_word3)\n",
    "    # sum freq of all the first 3 word combos of each phrase\n",
    "    total_count = float(sum(four_gram_model[word1_word2_word3].values()))\n",
    "    # get each fourth word and divide each frequency by total count\n",
    "    # for conditional probabilities\n",
    "    for word4 in four_gram_model[word1_word2_word3]:\n",
    "        # get the probability\n",
    "        # four_gram_model[word1_word2_word3]/total_count\n",
    "        four_gram_model[word1_word2_word3][word4] /= total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04838709677419355"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(dict(four_gram_model['the', 'movie', 'is']).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter three words: robert deniro and\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['jane', 'eddie']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_text = input('Please enter three words: ')\n",
    "user_tokens = tokenizer.tokenize(user_text)\n",
    "#print (user_tokens)\n",
    "prob_list = four_gram_model[user_tokens[0], user_tokens[1], user_tokens[2]].values()\n",
    "most_likely = max(prob_list)\n",
    "getKeysByValue(dict(four_gram_model[user_tokens[0], user_tokens[1], user_tokens[2]]), most_likely)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_gram_model = defaultdict(lambda: defaultdict(lambda: 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word1, word2, word3, word4, word5 in ngrams(tokens, 5):\n",
    "    # increase the count (frequency for tokens)\n",
    "    five_gram_model[word1, word2, word3, word4][word5] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word1_word2_word3_word4 in five_gram_model:\n",
    "    total_count = float(sum(five_gram_model[word1_word2_word3_word4].values()))\n",
    "    for word5 in five_gram_model[word1_word2_word3_word4]:\n",
    "        # get the probability\n",
    "        five_gram_model[word1_word2_word3_word4][word5] /= total_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter four words: oi oi oi oi\n",
      "4\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-a783b39d5165>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprob_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfive_gram_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmost_likely\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mgetKeysByValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfive_gram_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmost_likely\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "user_text = input('Please enter four words: ')\n",
    "user_tokens = tokenizer.tokenize(user_text)\n",
    "print (len(user_tokens))\n",
    "prob_list = five_gram_model[user_tokens[0], user_tokens[1], user_tokens[2], user_tokens[3]].values()\n",
    "most_likely = max(prob_list)\n",
    "getKeysByValue(dict(five_gram_model[user_tokens[0], user_tokens[1], user_tokens[2], user_tokens[3]]), most_likely)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would like to have it be ----\n",
    "'Enter 1 - 4 words please:'\n",
    "The model would tokenize the phrase \n",
    "According to length, apply the appropriate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. imagine a movie where joe piscopo is actually funny! maureen stapleton is a scene stealer. the moroni character is an absolute scream. watch for alan \"the skipper\" hale jr. as a police sgt.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter 1 to 4 words of your review: oi\n",
      "Sorry that word is not in our vocabulary\n"
     ]
    }
   ],
   "source": [
    "###### create a function that prompts for words...input\n",
    "###### into the proper model based on length\n",
    "###### if max( ) error - print a message\n",
    "\n",
    "# prompt user input\n",
    "user_text = input(\"Please enter 1 to 4 words of your review: \")\n",
    "user_tokens = tokenizer.tokenize(user_text)\n",
    "# check the length condition\n",
    "if len(user_tokens) > 4:\n",
    "    print(\"You have entered too many words, please enter 1 to 4 words of your review: \")\n",
    "elif len(user_tokens) == 1:\n",
    "    # bigram model\n",
    "    # use a try and except statement for nicer output of errors.\n",
    "    try:\n",
    "        prob_list = bigram_model.get(user_tokens[0]).values()\n",
    "        most_likely = max(prob_list)\n",
    "        next_word = getKeysByValue(dict(bigram_model[user_tokens[0]]), most_likely)\n",
    "        print('Your predicted phrase is:', user_tokens[0], next_word)\n",
    "    except AttributeError or ValueError:\n",
    "        print('Sorry that word is not in our vocabulary')\n",
    "elif len(user_tokens) == 2:\n",
    "    # trigram model\n",
    "    try:\n",
    "        prob_list = trigram_model[user_tokens[0], user_tokens[1]].values()\n",
    "        most_likely = max(prob_list)\n",
    "        next_word = getKeysByValue(dict(trigram_model[user_tokens[0], user_tokens[1]]), most_likely)\n",
    "        print('Your predicted phrase is:', user_tokens[0], user_tokens[1], next_word)\n",
    "    except ValueError:\n",
    "        print('That phrase is not in our vocabulary.')\n",
    "elif len(user_tokens) == 3:\n",
    "    # 4 gram model\n",
    "    try:\n",
    "        prob_list = four_gram_model[user_tokens[0], user_tokens[1], user_tokens[2]].values()\n",
    "        most_likely = max(prob_list)\n",
    "        next_word = getKeysByValue(dict(four_gram_model[user_tokens[0], user_tokens[1], user_tokens[2]]), most_likely)\n",
    "        print('Your predicted phrase is:', user_tokens[0], user_tokens[1], user_tokens[2], next_word)\n",
    "    except ValueError:\n",
    "        print('Sorry, that phrase is not in our vocabulary')\n",
    "elif len(user_tokens) == 4:\n",
    "    # 5 gram model\n",
    "    try:\n",
    "        prob_list = five_gram_model[user_tokens[0], user_tokens[1], user_tokens[2], user_tokens[3]].values()\n",
    "        most_likely = max(prob_list)\n",
    "        next_word = getKeysByValue(dict(five_gram_model[user_tokens[0], user_tokens[1], user_tokens[2], user_tokens[3]]), most_likely)\n",
    "        print('Your predicted phrase is:', user_tokens[0], user_tokens[1], user_tokens[2], user_tokens[3], next_word)\n",
    "    except ValueError:\n",
    "        print('Sorry that phrase is not in our vocabulary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_list = bigram_model.get('hi').values()\n",
    "most_likely = max(prob_list)\n",
    "getKeysByValue(dict(bigram_model['hi']), most_likely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
