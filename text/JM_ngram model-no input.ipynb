{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "from collections import defaultdict, Counter # for the model\n",
    "from nltk.util import ngrams \n",
    "import pandas as pd # dataframes \n",
    "import numpy as np \n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import glob # read multiple files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "12500\n",
      "12500\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "train_pos_filenames = glob.glob('data/aclImdb/train/pos/*.txt')\n",
    "train_neg_filenames = glob.glob('data/aclImdb/train/neg/*.txt')\n",
    "test_pos_filenames = glob.glob('data/aclImdb/test/pos/*.txt')\n",
    "test_neg_filenames = glob.glob('data/aclImdb/test/neg/*.txt')\n",
    "print(len(train_pos_filenames))\n",
    "print(len(train_neg_filenames))\n",
    "print(len(test_pos_filenames))\n",
    "print(len(test_neg_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_pos_text:\n",
      "bromwell high is a cartoon comedy. it ran at the same time as some other programs about school life, such as \"teachers\". my 35 years in the teaching profession lead me to believe that bromwell high's satire is much closer to reality than is \"teachers\". the scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools i knew and their students. when i saw the episode in which a student repeatedly tried to burn down the school, i immediately recalled ......... at .......... high. a classic line: inspector: i'm here to sack one of your teachers. student: welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn't!\n",
      "\n",
      "train_neg_text:\n",
      "story of a man who has unnatural feelings for a pig. starts out with a opening scene that is a terrific example of absurd comedy. a formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting. even those from the era should be turned off. the cryptic dialogue would make shakespeare seem easy to a third grader. on a technical level it's better than you might think with some good cinematography by future great vilmos zsigmond. future stars sally kirkland and frederic forrest can be seen briefly.\n",
      "\n",
      "test_pos_text:\n",
      "i went and saw this movie last night after being coaxed to by a few friends of mine. i'll admit that i was reluctant to see it because from what i knew of ashton kutcher he was only able to do comedy. i was wrong. kutcher played the character of jake fischer very well, and kevin costner played ben randall with such professionalism. the sign of a good movie is that it can toy with our emotions. this one did exactly that. the entire theater (which was sold out) was overcome by laughter during the first half of the movie, and were moved to tears during the second half. while exiting the theater i not only saw many women in tears, but many full grown men as well, trying desperately not to let anyone see them crying. this movie was great, and i suggest that you go see it before you judge.\n",
      "\n",
      "test_neg_text:\n",
      "once again mr. costner has dragged out a movie for far longer than necessary. aside from the terrific sea rescue sequences, of which there are very few i just did not care about any of the characters. most of us have ghosts in the closet, and costner's character are realized early on, and then forgotten until much later, by which time i did not care. the character we should really care about is a very cocky, overconfident ashton kutcher. the problem is he comes off as kid who thinks he's better than anyone else around him and shows no signs of a cluttered closet. his only obstacle appears to be winning over costner. finally when we are well past the half way point of this stinker, costner tells us all about kutcher's ghosts. we are told why kutcher is driven to be the best with no prior inkling or foreshadowing. no magic here, it was all i could do to keep from turning it off an hour in.\n",
      "Wall time: 21.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# read the contents of the train_pos files into a list (each list element is one review)\n",
    "# clean line breaks and html tags like <br\\>\n",
    "train_pos_text = []\n",
    "for filename in train_pos_filenames:\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\n|<\\w+\\s/>', '', text)\n",
    "        train_pos_text.append(text)\n",
    "print(\"train_pos_text:\")\n",
    "print(train_pos_text[0])\n",
    "\n",
    "\n",
    "# read the contents of the train_pos files into a list (each list element is one review)\n",
    "train_neg_text = []\n",
    "for filename in train_neg_filenames:\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\n|<\\w+\\s/>', '', text)\n",
    "        train_neg_text.append(text)\n",
    "print(\"\\ntrain_neg_text:\")\n",
    "print(train_neg_text[0])\n",
    "\n",
    "test_pos_text = []\n",
    "for filename in test_pos_filenames:\n",
    "    with open(filename, encoding = 'utf-8') as f:\n",
    "        text = f.read()\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\n|<\\w+\\s/>', '', text)\n",
    "        test_pos_text.append(text)\n",
    "print('\\ntest_pos_text:')\n",
    "print(test_pos_text[0])\n",
    "\n",
    "test_neg_text = []\n",
    "for filename in test_neg_filenames:\n",
    "    with open(filename, encoding = 'utf-8') as f:\n",
    "        text = f.read()\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\n|<\\w+\\s/>', '', text)\n",
    "        test_neg_text.append(text)\n",
    "print('\\ntest_neg_text:')\n",
    "print(test_neg_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11772360 tokens. \n",
      "There are 104138 unique tokens. \n",
      "Wall time: 18min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# combine all reviews into a long string to get more n-grams\n",
    "words = ''\n",
    "for rev in train_pos_text:\n",
    "    words += ' ' + rev\n",
    "\n",
    "for rev in train_neg_text:\n",
    "    words += ' ' + rev\n",
    "    \n",
    "for rev in test_pos_text:\n",
    "    words += ' ' + rev\n",
    "\n",
    "for rev in test_neg_text:\n",
    "    words += ' ' + rev\n",
    "\n",
    "# define tokenizer to get words\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# words has all the review as one string\n",
    "tokens = tokenizer.tokenize(words)\n",
    "print('There are {} tokens. '.format(len(tokens)))\n",
    "print('There are {} unique tokens. '.format(len(set(tokens))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the default dictionary - used for saving the model with dill library\n",
    "def default_dict():\n",
    "    return defaultdict(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the N-gram Models\n",
    "N = 2, 3, 4, 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the conditional probability of word2 given word1\n",
    "# P(word2|word1)\n",
    "def build_bigram_model():\n",
    "    bigram_model = defaultdict(default_dict) # create a model mold\n",
    "    # collect all bigrams for (w1, w2)\n",
    "    for word1, word2 in ngrams(tokens, 2):\n",
    "        # increase the count (frequency of tokens)\n",
    "        bigram_model[word1][word2] += 1\n",
    "    # compute the probability P(word2|word1)\n",
    "    for word1 in bigram_model:\n",
    "        # get total count of bigrams with word1\n",
    "        total_count = float(sum(bigram_model[word1].values()))\n",
    "        for word2 in bigram_model[word1]:\n",
    "            # number of bigrams (word1 word2)/total\n",
    "            bigram_model[word1][word2] /= total_count\n",
    "    return bigram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# build the model\n",
    "bigram_model = build_bigram_model()\n",
    "import dill\n",
    "with open('models/bigram_model.p', 'wb') as file:\n",
    "    dill.dump(bigram_model, file, protocol=dill.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict the next word based on bigram model\n",
    "def bigram_predict_next_word(word):\n",
    "    '''\n",
    "    word: a list of token\n",
    "    '''\n",
    "    if len(bigram_model[word[0]]) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        prob_list = bigram_model[word[0]].values()\n",
    "        # find the max prob\n",
    "        most_likely = max(prob_list)\n",
    "        #print(most_likely)\n",
    "        # predicted words\n",
    "        pred_words = [word for word, prob in bigram_model[word[0]].items() if prob == most_likely]\n",
    "#         pred_word = random.choice(pred_words)\n",
    "    return pred_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pitt'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_predict_next_word(tokenizer.tokenize('brad'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trigram_model():\n",
    "    trigram_model = defaultdict(default_dict) # create a model mold\n",
    "    # collect trigrams for word1, word2, word3\n",
    "    for word1, word2, word3 in ngrams(tokens , 3):\n",
    "        # increase the count \n",
    "        trigram_model[word1, word2][word3] += 1\n",
    "        # compute the probability P(word1, word|word3)\n",
    "    for word1_word2 in trigram_model:\n",
    "        # get total count of trigrams with word1 word2\n",
    "        total_count = float(sum(trigram_model[word1_word2].values()))\n",
    "        for word3 in trigram_model[word1_word2]:\n",
    "            # number of trigrams/total\n",
    "            trigram_model[word1_word2][word3] /= total_count\n",
    "    return trigram_model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# define the model\n",
    "trigram_model = build_trigram_model()\n",
    "import dill\n",
    "with open('models/trigram_model.p', 'wb') as file:\n",
    "    dill.dump(trigram_model, file, protocol=dill.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict next word with trigram model\n",
    "def trigram_predict_next_word(words):\n",
    "    '''\n",
    "    words: a list of token\n",
    "    '''\n",
    "    if len(trigram_model[words[0], words[1]]) == 0:\n",
    "        last_word = words[-1]\n",
    "        return bigram_predict_next_word(last_word)\n",
    "    else:\n",
    "        # get probabilities of next word\n",
    "        prob_list = trigram_model[words[0], words[1]].values()\n",
    "        # find the max prob\n",
    "        most_likely = max(prob_list)\n",
    "        # predicted words\n",
    "        pred_words = [word for word, prob in trigram_model[words[0], words[1]].items() if prob == most_likely]\n",
    "#         pred_word = random.choice(pred_words)\n",
    "    return pred_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'that'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_predict_next_word(tokenizer.tokenize('funny movie'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fourgram_model():\n",
    "    fourgram_model = defaultdict(default_dict) # create a model mold\n",
    "    # collect 4-grams for word1, word2, word3, word4\n",
    "    for word1, word2, word3, word4 in ngrams(tokens, 4):\n",
    "        # increase the count\n",
    "        fourgram_model[word1, word2, word3][word4] += 1\n",
    "        # compute the probability P(word1, word2, word3|word4)\n",
    "    for word1_word2_word3 in fourgram_model:\n",
    "        # get total count of 4grams with word1, word2, word3\n",
    "        total_count = float(sum(fourgram_model[word1_word2_word3].values()))\n",
    "        for word4 in fourgram_model[word1_word2_word3]:\n",
    "            # number of 4grams/total\n",
    "            fourgram_model[word1_word2_word3][word4] /= total_count\n",
    "    return fourgram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fourgram_model = build_fourgram_model()\n",
    "import dill\n",
    "with open('models/fourgram_model.p', 'wb') as file:\n",
    "    dill.dump(fourgram_model, file, protocol=dill.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourgram_predict_next_word(words):\n",
    "    '''\n",
    "    words: a list of token\n",
    "    '''\n",
    "    if len(fourgram_model[words[0], words[1], words[2]]) == 0:\n",
    "        last_two_words = words[-2:] \n",
    "        return trigram_predict_next_word(last_two_words)\n",
    "    else:\n",
    "        # get probabilities of next word\n",
    "        prob_list = fourgram_model[words[0], words[1], words[2]].values()\n",
    "        # find max prob\n",
    "        most_likely = max(prob_list)\n",
    "        # get the predicted word(s)\n",
    "        pred_words = [word for word, prob in fourgram_model[words[0], words[1], words[2]].items() if prob == most_likely]\n",
    "        pred_word = random.choice(pred_words)\n",
    "    return pred_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourgram_predict_next_word(tokenizer.tokenize('funny movie that'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build 5-gram model\n",
    "def build_fivegram_model():\n",
    "    fivegram_model = defaultdict(default_dict)\n",
    "    # collect 5-grams\n",
    "    for word1, word2, word3, word4, word5, in ngrams(tokens, 5):\n",
    "        fivegram_model[word1, word2, word3, word4][word5] += 1\n",
    "    # compute the probability P(word1, word2, word3, word4|word5)\n",
    "    for word1_word2_word3_word4 in fivegram_model:\n",
    "        # get total count of 5-grams\n",
    "        total_count = float(sum(fivegram_model[word1_word2_word3_word4].values()))\n",
    "        for word5 in fivegram_model[word1_word2_word3_word4]:\n",
    "            # number of 5grams/total\n",
    "            fivegram_model[word1_word2_word3_word4][word5] /= total_count\n",
    "    return fivegram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fivegram_model = build_fivegram_model()\n",
    "import dill\n",
    "with open('models/fivegram_model.p', 'wb') as file:\n",
    "    dill.dump(fivegram_model, file, protocol=dill.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict next word with 5-gram model\n",
    "def fivegram_predict_next_word(words):\n",
    "    '''\n",
    "    words: a list of token\n",
    "    '''\n",
    "    if len(fivegram_model[words[0], words[1], words[2], words[3]]) == 0:\n",
    "        last_three_words = words[-3:]\n",
    "        return fourgram_predict_next_word(last_three_words)\n",
    "    else:\n",
    "        # get probabilities of next word\n",
    "        prob_list = fivegram_model[words[0], words[1], words[2], words[3]].values()\n",
    "        # find max prob\n",
    "        most_likely = max(prob_list)\n",
    "        # predicted words\n",
    "        pred_words = [w for w, p in fivegram_model[words[0], words[1], words[2], words[3]].items() if p == most_likely]\n",
    "#         pred_word = random.choice(pred_words)\n",
    "    return pred_words[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cycle through the models to help return a predicted word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Concept Demo\n",
    "# text = 'first second third fourth fifth sixth'\n",
    "\n",
    "# def ngram_prediction(words):\n",
    "#     \"\"\"\n",
    "#     expect: a raw string of text\n",
    "#     modify: tokenize the string and decide to start with which model.\n",
    "#             1. if the string has 4 tokens or above, take the last four tokens as input to fivegram_model\n",
    "#             2. if the string has 3 tokens, take the list of tokens as input to fourgram_model\n",
    "#             3. if the string has 2 tokens, take the list of tokens as input to trigram_model\n",
    "#             4. if the string has 1 tokens, take the token as input to trigram_model\n",
    "#     return:\n",
    "#     \"\"\"\n",
    "#     # tokenize the words\n",
    "#     tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#     user_tokens = tokenizer.tokenize(words)\n",
    "#     if len(user_tokens) >= 4:\n",
    "#         return = user_tokens[-4:] # take the last four tokens\n",
    "#     elif len(user_tokens) == 3:\n",
    "#         return = user_tokens\n",
    "#     elif len(user_tokens) == 2:\n",
    "#         return = user_tokens\n",
    "#     elif len(user_tokens) == 1:\n",
    "#         return = user_tokens\n",
    "\n",
    "# ngram_prediction(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_prediction(text):\n",
    "    \"\"\"\n",
    "    expect: a raw string of text\n",
    "    modify: tokenize the string and # check the length to decide to start with which model.\n",
    "            1. if the string has 4 tokens or above, take the last four tokens as input to fivegram_model\n",
    "            2. if the string has 3 tokens, take the list of tokens as input to fourgram_model\n",
    "            3. if the string has 2 tokens, take the list of tokens as input to trigram_model\n",
    "            4. if the string has 1 token, take the token as input to bigram_model\n",
    "    return: predicted word\n",
    "    \"\"\"\n",
    "    # tokenize the words\n",
    "    user_tokens = tokenizer.tokenize(text)\n",
    "    if len(user_tokens) >= 4:\n",
    "        try:\n",
    "            return fivegram_predict_next_word(user_tokens[-4:]) # take the last four tokens\n",
    "        except:    \n",
    "            return fivegram_predict_next_word(user_tokens) # take the four tokens\n",
    "    elif len(user_tokens) == 3:\n",
    "        return fourgram_predict_next_word(user_tokens)\n",
    "    elif len(user_tokens) == 2:\n",
    "        return trigram_predict_next_word(user_tokens)\n",
    "    elif len(user_tokens) == 1:\n",
    "        return bigram_predict_next_word(user_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it', 'is', 'funny', 'movie', 'that']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('it is funny movie that')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.99 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ngram_prediction('it is funny movie that')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'also'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ngram_prediction('a funny movie that')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
