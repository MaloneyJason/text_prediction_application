{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "from collections import defaultdict, Counter # for the model\n",
    "from nltk.util import ngrams \n",
    "import pandas as pd # dataframes \n",
    "import numpy as np \n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import glob # read multiple files\n",
    "import nltk\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "12500\n",
      "12500\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "train_pos_filenames = glob.glob('/Users/jasonmaloney/Documents/Syracuse/IST 736 Text Mining/Text Mining Project/aclImdb/train/pos/*.txt')\n",
    "train_neg_filenames = glob.glob('/Users/jasonmaloney/Documents/Syracuse/IST 736 Text Mining/Text Mining Project/aclImdb/train/neg/*.txt')\n",
    "test_pos_filenames = glob.glob('/Users/jasonmaloney/Documents/Syracuse/IST 736 Text Mining/Text Mining Project/aclImdb/test/pos/*.txt')\n",
    "test_neg_filenames = glob.glob('/Users/jasonmaloney/Documents/Syracuse/IST 736 Text Mining/Text Mining Project/aclImdb/test/neg/*.txt')\n",
    "print(len(train_pos_filenames))\n",
    "print(len(train_neg_filenames))\n",
    "print(len(test_pos_filenames))\n",
    "print(len(test_neg_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 10 µs\n",
      "train_pos_text:\n",
      "for a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. imagine a movie where joe piscopo is actually funny! maureen stapleton is a scene stealer. the moroni character is an absolute scream. watch for alan \"the skipper\" hale jr. as a police sgt.\n",
      "\n",
      "train_neg_text:\n",
      "working with one of the best shakespeare sources, this film manages to be creditable to it's source, whilst still appealing to a wider audience.branagh steals the film from under fishburne's nose, and there's a talented cast on good form.\n",
      "\n",
      "test_pos_text:\n",
      "based on an actual story, john boorman shows the struggle of an american doctor, whose husband and son were murdered and she was continually plagued with her loss. a holiday to burma with her sister seemed like a good idea to get away from it all, but when her passport was stolen in rangoon, she could not leave the country with her sister, and was forced to stay back until she could get i.d. papers from the american embassy. to fill in a day before she could fly out, she took a trip into the countryside with a tour guide. \"i tried finding something in those stone statues, but nothing stirred in me. i was stone myself.\" suddenly all hell broke loose and she was caught in a political revolt. just when it looked like she had escaped and safely boarded a train, she saw her tour guide get beaten and shot. in a split second she decided to jump from the moving train and try to rescue him, with no thought of herself. continually her life was in danger. here is a woman who demonstrated spontaneous, selfless charity, risking her life to save another. patricia arquette is beautiful, and not just to look at; she has a beautiful heart. this is an unforgettable story. \"we are taught that suffering is the one promise that life always keeps.\"\n",
      "\n",
      "test_neg_text:\n",
      "alan rickman & emma thompson give good performances with southern/new orleans accents in this detective flick. it's worth seeing for their scenes- and rickman's scene with hal holbrook. these three actors mannage to entertain us no matter what the movie, it seems. the plot for the movie shows potential, but one gets the impression in watching the film that it was not pulled off as well as it could have been. the fact that it is cluttered by a rather uninteresting subplot and mostly uninteresting kidnappers really muddles things. the movie is worth a view- if for nothing more than entertaining performances by rickman, thompson, and holbrook.\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# read the contents of the train_pos files into a list (each list element is one review)\n",
    "# clean line breaks and html tags like <br\\>\n",
    "train_pos_text = []\n",
    "for filename in train_pos_filenames:\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\n|<\\w+\\s/>', '', text)\n",
    "        train_pos_text.append(text)\n",
    "print(\"train_pos_text:\")\n",
    "print(train_pos_text[0])\n",
    "\n",
    "\n",
    "# read the contents of the train_pos files into a list (each list element is one review)\n",
    "train_neg_text = []\n",
    "for filename in train_neg_filenames:\n",
    "    with open(filename, encoding = 'utf-8') as f:\n",
    "        text = f.read()\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\n|<\\w+\\s/>', '', text)\n",
    "        train_neg_text.append(text)\n",
    "print(\"\\ntrain_neg_text:\")\n",
    "print(train_neg_text[0])\n",
    "\n",
    "test_pos_text = []\n",
    "for filename in test_pos_filenames:\n",
    "    with open(filename, encoding = 'utf-8') as f:\n",
    "        text = f.read()\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\n|<\\w+\\s/>', '', text)\n",
    "        test_pos_text.append(text)\n",
    "print('\\ntest_pos_text:')\n",
    "print(test_pos_text[0])\n",
    "\n",
    "test_neg_text = []\n",
    "for filename in test_neg_filenames:\n",
    "    with open(filename, encoding = 'utf-8') as f:\n",
    "        text = f.read()\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\n|<\\w+\\s/>', '', text)\n",
    "        test_neg_text.append(text)\n",
    "print('\\ntest_neg_text:')\n",
    "print(test_neg_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11772360 tokens. \n",
      "There are 104138 unique tokens. \n"
     ]
    }
   ],
   "source": [
    "# combine all reviews into a long string to get more n-grams\n",
    "words = ''\n",
    "for rev in train_pos_text:\n",
    "    words += ' ' + rev\n",
    "\n",
    "for rev in train_neg_text:\n",
    "    words += ' ' + rev\n",
    "    \n",
    "for rev in test_pos_text:\n",
    "    words += ' ' + rev\n",
    "\n",
    "for rev in test_neg_text:\n",
    "    words += ' ' + rev\n",
    "\n",
    "# define tokenizer to get words\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# words has all the review as one string\n",
    "tokens = tokenizer.tokenize(words)\n",
    "print('There are {} tokens. '.format(len(tokens)))\n",
    "print('There are {} unique tokens. '.format(len(set(tokens))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the default dictionary - used for saving the model with dill library\n",
    "def default_dict():\n",
    "    return defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get frequency distributions\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "tag_tokens = nltk.pos_tag(tokens)\n",
    "tag_fd = nltk.FreqDist(tag for (word, tag) in tag_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram POS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the conditional probability of word2 given word1\n",
    "# P(pos2|pos1)\n",
    "def build_tag_bigram_model():\n",
    "    tag_bigram_model = defaultdict(default_dict) # create a model mold\n",
    "    # collect all bigrams and the POS for each word\n",
    "    # tag_tokens has tuples of (word, POS)\n",
    "    for tuple1, tuple2 in ngrams(tag_tokens, 2):\n",
    "        # increase the count (frequency of tokens)\n",
    "        tag_bigram_model[tuple1[1]][tuple2[1]] += 1\n",
    "    # compute the probability P(word2|word1)\n",
    "    for pos1 in tag_bigram_model:\n",
    "        # get total count of bigrams with word1\n",
    "        total_count = float(sum(tag_bigram_model[pos1].values()))\n",
    "        for pos2 in tag_bigram_model[pos1]:\n",
    "            # number of bigrams (word1 word2)/total\n",
    "            tag_bigram_model[pos1][pos2] /= total_count\n",
    "    return tag_bigram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_bigram_model = build_tag_bigram_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict the next word based on bigram model\n",
    "def bigram_predict_next_pos(first_pos):\n",
    "    if len(tag_bigram_model[first_pos]) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        # get probabilities of next POS given user input\n",
    "        prob_list = tag_bigram_model[first_pos].values()\n",
    "        #print(prob_list)\n",
    "        # find the max prob\n",
    "        most_likely = max(prob_list)\n",
    "        #print(most_likely)\n",
    "        # predicted words\n",
    "        pred_pos = [pos for pos, prob in tag_bigram_model[first_pos].items() if prob == most_likely]\n",
    "    return pred_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DT']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_predict_next_pos('VBD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram POS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the conditional probability of word2 given word1\n",
    "# P(pos3|pos1, pos2)\n",
    "def build_tag_trigram_model():\n",
    "    tag_trigram_model = defaultdict(default_dict) # create a model mold\n",
    "    # collect all bigrams and the POS for each word\n",
    "    # tag_tokens has tuples of (word, POS)\n",
    "    for tuple1, tuple2, tuple3 in ngrams(tag_tokens, 3):\n",
    "        # increase the count (frequency of tokens)\n",
    "        tag_trigram_model[tuple1[1], tuple2[1]][tuple3[1]] += 1\n",
    "    # compute the probability P(word2|word1)\n",
    "    for pos1, pos2 in tag_trigram_model:\n",
    "        # get total count of bigrams with word1\n",
    "        total_count = float(sum(tag_trigram_model[pos1, pos2].values()))\n",
    "        for pos3 in tag_trigram_model[pos1, pos2]:\n",
    "            # number of bigrams (word1 word2)/total\n",
    "            tag_trigram_model[pos1, pos2][pos3] /= total_count\n",
    "    return tag_trigram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_trigram_model = build_tag_trigram_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict the next word based on bigram model\n",
    "def trigram_predict_next_pos(first_pos, second_pos):\n",
    "    if len(tag_trigram_model[first_pos, second_pos]) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        # get probabilities of next POS given user input\n",
    "        prob_list = tag_trigram_model[first_pos, second_pos].values()\n",
    "        #print(prob_list)\n",
    "        # find the max prob\n",
    "        most_likely = max(prob_list)\n",
    "        #print(most_likely)\n",
    "        # predicted words\n",
    "        pred_pos = [pos for pos, prob in tag_trigram_model[first_pos, second_pos].items() if prob == most_likely]\n",
    "    return pred_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NN']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_predict_next_pos('VBD', 'DT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the most common word of all POS categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NN', 'DT', 'IN', 'JJ', 'RB', 'NNS', 'VBZ', 'PRP', 'CC', 'VB', 'VBP', 'VBD', 'TO', 'VBN', 'VBG', 'PRP$', 'CD', 'MD', 'WP', 'WRB', 'WDT', 'RP', 'JJS', 'JJR', 'EX', 'RBR', 'PDT', 'RBS', 'FW', 'NNP', 'WP$', 'UH', '$', 'NNPS', \"''\", 'SYM', 'POS', '``']\n"
     ]
    }
   ],
   "source": [
    "# list of all the POS in the corpus\n",
    "pos_tag_list = []\n",
    "for pos, count in tag_fd.most_common():\n",
    "    pos_tag_list.append(pos)\n",
    "#print(pos_tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('the', 'DT'), 667709),\n",
       " (('and', 'CC'), 324361),\n",
       " (('a', 'DT'), 322891),\n",
       " (('of', 'IN'), 289376),\n",
       " (('to', 'TO'), 268076),\n",
       " (('is', 'VBZ'), 211052),\n",
       " (('it', 'PRP'), 190801),\n",
       " (('in', 'IN'), 186728),\n",
       " (('this', 'DT'), 150916),\n",
       " (('was', 'VBD'), 95584)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get counts of words with POS tags\n",
    "word_tag_fd = nltk.FreqDist(tag_tokens)\n",
    "#word_tag_fd.most_common(10)\n",
    "# word_tag_fd is a tuple ((word, POS), count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NN': 'i',\n",
       " 'DT': 'the',\n",
       " 'IN': 'of',\n",
       " 'JJ': 'i',\n",
       " 'RB': 'not',\n",
       " 'NNS': 'people',\n",
       " 'VBZ': 'is',\n",
       " 'PRP': 'it',\n",
       " 'CC': 'and',\n",
       " 'VB': 'be',\n",
       " 'VBP': 'are',\n",
       " 'VBD': 'was',\n",
       " 'TO': 'to',\n",
       " 'VBN': 'been',\n",
       " 'VBG': 'being',\n",
       " 'PRP$': 'his',\n",
       " 'CD': 'one',\n",
       " 'MD': 'can',\n",
       " 'WP': 'who',\n",
       " 'WRB': 'when',\n",
       " 'WDT': 'that',\n",
       " 'RP': 'up',\n",
       " 'JJS': 'best',\n",
       " 'JJR': 'more',\n",
       " 'EX': 'there',\n",
       " 'RBR': 'more',\n",
       " 'PDT': 'all',\n",
       " 'RBS': 'most',\n",
       " 'FW': 'etc',\n",
       " 'NNP': 'x',\n",
       " 'WP$': 'whose',\n",
       " 'UH': 'oh',\n",
       " '$': 'zombi',\n",
       " 'NNPS': 'republicans',\n",
       " \"''\": 'marry',\n",
       " 'SYM': 'b',\n",
       " 'POS': 's',\n",
       " '``': 'neighbour'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the top word of each part of speech tag\n",
    "# empty dictionary\n",
    "pos_word_dict = {}\n",
    "# iterate through each part of speech\n",
    "for pos in pos_tag_list:\n",
    "    # keep only the most common word for each POS tag - create a list and keep the first element\n",
    "    # first element is most common because .most_common() is sorted by count\n",
    "    # word_tag_fd is a tuple ((word, POS), count)\n",
    "    # word = word_tag[0], pos = word_tag[1]\n",
    "    pos_word_dict[pos] = [word_tag[0] for (word_tag, count) in word_tag_fd.most_common() if word_tag[1] == pos][0]\n",
    "    #print([wt[0] for (wt, _) in word_tag_fd.most_common() if wt[1] == pos][0], '---', pos)\n",
    "pos_word_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Text Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the conditional probability of word2 given word1\n",
    "# P(word2|word1)\n",
    "def build_bigram_model():\n",
    "    bigram_model = defaultdict(default_dict) # create a model mold\n",
    "    # collect all bigrams for (w1, w2)\n",
    "    for word1, word2 in ngrams(tokens, 2):\n",
    "        # increase the count (frequency of tokens)\n",
    "        bigram_model[word1][word2] += 1\n",
    "    # compute the probability P(word2|word1)\n",
    "    for word1 in bigram_model:\n",
    "        # get total count of bigrams with word1\n",
    "        total_count = float(sum(bigram_model[word1].values()))\n",
    "        for word2 in bigram_model[word1]:\n",
    "            # number of bigrams (word1 word2)/total\n",
    "            bigram_model[word1][word2] /= total_count\n",
    "    return bigram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "bigram_model = build_bigram_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict the next word based on bigram model\n",
    "def bigram_predict_next_word(first_word):\n",
    "    if len(bigram_model[first_word]) == 0:\n",
    "        # get POS of the input word\n",
    "        token = tokenizer.tokenize(first_word)\n",
    "        # get the part of speech [(word, POS)]\n",
    "        pos = nltk.pos_tag(token)[0][1]\n",
    "        # predict next word POS - returns a list\n",
    "        pred_pos = bigram_predict_next_pos(pos)\n",
    "        # get the most common word of predicted POS\n",
    "        pred_words = pos_word_dict[pred_pos[0]]\n",
    "    \n",
    "    else:\n",
    "        # tokenize user input\n",
    "        user_tokens = tokenizer.tokenize(first_word)\n",
    "        #print('input:', user_tokens[0])\n",
    "        # get probabilities of next word given user input\n",
    "        prob_list = bigram_model[user_tokens[0]].values()\n",
    "        #print(prob_list)\n",
    "        # find the max prob\n",
    "        most_likely = max(prob_list)\n",
    "        #print(most_likely)\n",
    "        # predicted words\n",
    "        pred_words = [word for word, prob in bigram_model[user_tokens[0]].items() if prob == most_likely]\n",
    "        # if there is more than one predicted word, return the most frequent in the corpus\n",
    "        if len(pred_words) > 1:\n",
    "            word_dict = {}\n",
    "            # get the frequency counts of the predicted word\n",
    "            for word in pred_words:\n",
    "                word_dict[word] = fdist[word]\n",
    "            # get the word with max freqency\n",
    "            for word,count in word_dict.items():\n",
    "                if count == max(word_dict.values()):\n",
    "                    pred_words = word\n",
    "            \n",
    "    return pred_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'of'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_predict_next_word('snate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram Text Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trigram_model():\n",
    "    trigram_model = defaultdict(default_dict) # create a model mold\n",
    "    # collect trigrams for word1, word2, word3\n",
    "    for word1, word2, word3 in ngrams(tokens , 3):\n",
    "        # increase the count \n",
    "        trigram_model[word1, word2][word3] += 1\n",
    "        # compute the probability P(word1, word|word3)\n",
    "    for word1_word2 in trigram_model:\n",
    "        # get total count of trigrams with word1 word2\n",
    "        total_count = float(sum(trigram_model[word1_word2].values()))\n",
    "        for word3 in trigram_model[word1_word2]:\n",
    "            # number of trigrams/total\n",
    "            trigram_model[word1_word2][word3] /= total_count\n",
    "    return trigram_model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "trigram_model = build_trigram_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict next word with trigram model\n",
    "def trigram_predict_next_word(two_words):\n",
    "    # tokenize user input\n",
    "    user_tokens = tokenizer.tokenize(two_words)\n",
    "    #print(user_tokens)\n",
    "    # if input is not in the vocabulary - use POS\n",
    "    if len(trigram_model[user_tokens[0], user_tokens[1]]) == 0:\n",
    "        # get POS of the input words\n",
    "        tagged_words = nltk.pos_tag(user_tokens)\n",
    "        pos1, pos2 = tagged_words[0][1], tagged_words[1][1]\n",
    "        # predict next word POS - needs two arguments\n",
    "        pred_pos = trigram_predict_next_pos(pos1, pos2)\n",
    "        #print(pred_pos)\n",
    "        # get the most common word of predicted POS\n",
    "        pred_words = pos_word_dict[pred_pos[0]]\n",
    "        \n",
    "    else:\n",
    "        # get probabilities of next word\n",
    "        prob_list = trigram_model[user_tokens[0], user_tokens[1]].values()\n",
    "        # find the max prob\n",
    "        #print(prob_list)\n",
    "        most_likely = max(prob_list)\n",
    "        #print(most_likely)\n",
    "        # predicted words\n",
    "        pred_words = [word for word, prob in trigram_model[user_tokens[0], user_tokens[1]].items() if prob == most_likely]\n",
    "        # if there are more than one word in pred_words\n",
    "        if len(pred_words) > 1:\n",
    "            # create a dictionary to pair up words and counts\n",
    "            word_dict = {}\n",
    "            # iterate through all pred_words choices\n",
    "            for word in pred_words:\n",
    "                # add word:count to dictionary\n",
    "                word_dict[word] = fdist[word]\n",
    "            for word,count in word_dict.items():\n",
    "                # find the word that has the highest count - that is the word to return\n",
    "                if count == max(word_dict.values()):\n",
    "                    pred_words = word\n",
    "    return pred_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'can'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_predict_next_word('hey you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
