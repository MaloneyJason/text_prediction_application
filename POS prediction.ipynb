{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "from collections import defaultdict, Counter # for the model\n",
    "from nltk.util import ngrams \n",
    "import pandas as pd # dataframes \n",
    "import numpy as np \n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import glob # read multiple files\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "12500\n",
      "12500\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "train_pos_filenames = glob.glob('/Users/jasonmaloney/Documents/Syracuse/IST 736 Text Mining/Text Mining Project/aclImdb/train/pos/*.txt')\n",
    "train_neg_filenames = glob.glob('/Users/jasonmaloney/Documents/Syracuse/IST 736 Text Mining/Text Mining Project/aclImdb/train/neg/*.txt')\n",
    "test_pos_filenames = glob.glob('/Users/jasonmaloney/Documents/Syracuse/IST 736 Text Mining/Text Mining Project/aclImdb/test/pos/*.txt')\n",
    "test_neg_filenames = glob.glob('/Users/jasonmaloney/Documents/Syracuse/IST 736 Text Mining/Text Mining Project/aclImdb/test/neg/*.txt')\n",
    "print(len(train_pos_filenames))\n",
    "print(len(train_neg_filenames))\n",
    "print(len(test_pos_filenames))\n",
    "print(len(test_neg_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 10 µs\n",
      "train_pos_text:\n",
      "for a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. imagine a movie where joe piscopo is actually funny! maureen stapleton is a scene stealer. the moroni character is an absolute scream. watch for alan \"the skipper\" hale jr. as a police sgt.\n",
      "\n",
      "train_neg_text:\n",
      "working with one of the best shakespeare sources, this film manages to be creditable to it's source, whilst still appealing to a wider audience.branagh steals the film from under fishburne's nose, and there's a talented cast on good form.\n",
      "\n",
      "test_pos_text:\n",
      "based on an actual story, john boorman shows the struggle of an american doctor, whose husband and son were murdered and she was continually plagued with her loss. a holiday to burma with her sister seemed like a good idea to get away from it all, but when her passport was stolen in rangoon, she could not leave the country with her sister, and was forced to stay back until she could get i.d. papers from the american embassy. to fill in a day before she could fly out, she took a trip into the countryside with a tour guide. \"i tried finding something in those stone statues, but nothing stirred in me. i was stone myself.\" suddenly all hell broke loose and she was caught in a political revolt. just when it looked like she had escaped and safely boarded a train, she saw her tour guide get beaten and shot. in a split second she decided to jump from the moving train and try to rescue him, with no thought of herself. continually her life was in danger. here is a woman who demonstrated spontaneous, selfless charity, risking her life to save another. patricia arquette is beautiful, and not just to look at; she has a beautiful heart. this is an unforgettable story. \"we are taught that suffering is the one promise that life always keeps.\"\n",
      "\n",
      "test_neg_text:\n",
      "alan rickman & emma thompson give good performances with southern/new orleans accents in this detective flick. it's worth seeing for their scenes- and rickman's scene with hal holbrook. these three actors mannage to entertain us no matter what the movie, it seems. the plot for the movie shows potential, but one gets the impression in watching the film that it was not pulled off as well as it could have been. the fact that it is cluttered by a rather uninteresting subplot and mostly uninteresting kidnappers really muddles things. the movie is worth a view- if for nothing more than entertaining performances by rickman, thompson, and holbrook.\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# read the contents of the train_pos files into a list (each list element is one review)\n",
    "# clean line breaks and html tags like <br\\>\n",
    "train_pos_text = []\n",
    "for filename in train_pos_filenames:\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\n|<\\w+\\s/>', '', text)\n",
    "        train_pos_text.append(text)\n",
    "print(\"train_pos_text:\")\n",
    "print(train_pos_text[0])\n",
    "\n",
    "\n",
    "# read the contents of the train_pos files into a list (each list element is one review)\n",
    "train_neg_text = []\n",
    "for filename in train_neg_filenames:\n",
    "    with open(filename, encoding = 'utf-8') as f:\n",
    "        text = f.read()\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\n|<\\w+\\s/>', '', text)\n",
    "        train_neg_text.append(text)\n",
    "print(\"\\ntrain_neg_text:\")\n",
    "print(train_neg_text[0])\n",
    "\n",
    "test_pos_text = []\n",
    "for filename in test_pos_filenames:\n",
    "    with open(filename, encoding = 'utf-8') as f:\n",
    "        text = f.read()\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\n|<\\w+\\s/>', '', text)\n",
    "        test_pos_text.append(text)\n",
    "print('\\ntest_pos_text:')\n",
    "print(test_pos_text[0])\n",
    "\n",
    "test_neg_text = []\n",
    "for filename in test_neg_filenames:\n",
    "    with open(filename, encoding = 'utf-8') as f:\n",
    "        text = f.read()\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\n|<\\w+\\s/>', '', text)\n",
    "        test_neg_text.append(text)\n",
    "print('\\ntest_neg_text:')\n",
    "print(test_neg_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11772360 tokens. \n",
      "There are 104138 unique tokens. \n"
     ]
    }
   ],
   "source": [
    "# combine all reviews into a long string to get more n-grams\n",
    "words = ''\n",
    "for rev in train_pos_text:\n",
    "    words += ' ' + rev\n",
    "\n",
    "for rev in train_neg_text:\n",
    "    words += ' ' + rev\n",
    "    \n",
    "for rev in test_pos_text:\n",
    "    words += ' ' + rev\n",
    "\n",
    "for rev in test_neg_text:\n",
    "    words += ' ' + rev\n",
    "\n",
    "# define tokenizer to get words\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# words has all the review as one string\n",
    "tokens = tokenizer.tokenize(words)\n",
    "print('There are {} tokens. '.format(len(tokens)))\n",
    "print('There are {} unique tokens. '.format(len(set(tokens))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the default dictionary - used for saving the model with dill library\n",
    "def default_dict():\n",
    "    return defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get frequency distributions\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "tag_tokens = nltk.pos_tag(tokens)\n",
    "tag_fd = nltk.FreqDist(tag for (word, tag) in tag_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram POS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the conditional probability of word2 given word1\n",
    "# P(pos2|pos1)\n",
    "def build_tag_bigram_model():\n",
    "    tag_bigram_model = defaultdict(default_dict) # create a model mold\n",
    "    # collect all bigrams and the POS for each word\n",
    "    # tag_tokens has tuples of (word, POS)\n",
    "    for tuple1, tuple2 in ngrams(tag_tokens, 2):\n",
    "        # increase the count (frequency of tokens)\n",
    "        tag_bigram_model[tuple1[1]][tuple2[1]] += 1\n",
    "    # compute the probability P(word2|word1)\n",
    "    for pos1 in tag_bigram_model:\n",
    "        # get total count of bigrams with word1\n",
    "        total_count = float(sum(tag_bigram_model[pos1].values()))\n",
    "        for pos2 in tag_bigram_model[pos1]:\n",
    "            # number of bigrams (word1 word2)/total\n",
    "            tag_bigram_model[pos1][pos2] /= total_count\n",
    "    return tag_bigram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_bigram_model = build_tag_bigram_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict the next word based on bigram model\n",
    "def bigram_predict_next_pos(first_pos):\n",
    "    if len(tag_bigram_model[first_pos]) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        # get probabilities of next POS given user input\n",
    "        prob_list = tag_bigram_model[first_pos].values()\n",
    "        #print(prob_list)\n",
    "        # find the max prob\n",
    "        most_likely = max(prob_list)\n",
    "        #print(most_likely)\n",
    "        # predicted words\n",
    "        pred_pos = [pos for pos, prob in tag_bigram_model[first_pos].items() if prob == most_likely]\n",
    "    return pred_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DT']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_predict_next_pos('VBD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram POS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the conditional probability of word2 given word1\n",
    "# P(pos3|pos1, pos2)\n",
    "def build_tag_trigram_model():\n",
    "    tag_trigram_model = defaultdict(default_dict) # create a model mold\n",
    "    # collect all bigrams and the POS for each word\n",
    "    # tag_tokens has tuples of (word, POS)\n",
    "    for tuple1, tuple2, tuple3 in ngrams(tag_tokens, 3):\n",
    "        # increase the count (frequency of tokens)\n",
    "        tag_trigram_model[tuple1[1], tuple2[1]][tuple3[1]] += 1\n",
    "    # compute the probability P(word2|word1)\n",
    "    for pos1, pos2 in tag_trigram_model:\n",
    "        # get total count of bigrams with word1\n",
    "        total_count = float(sum(tag_trigram_model[pos1, pos2].values()))\n",
    "        for pos3 in tag_trigram_model[pos1, pos2]:\n",
    "            # number of bigrams (word1 word2)/total\n",
    "            tag_trigram_model[pos1, pos2][pos3] /= total_count\n",
    "    return tag_trigram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_trigram_model = build_tag_trigram_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict the next word based on bigram model\n",
    "def trigram_predict_next_pos(first_pos, second_pos):\n",
    "    if len(tag_trigram_model[first_pos, second_pos]) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        # get probabilities of next POS given user input\n",
    "        prob_list = tag_trigram_model[first_pos, second_pos].values()\n",
    "        #print(prob_list)\n",
    "        # find the max prob\n",
    "        most_likely = max(prob_list)\n",
    "        #print(most_likely)\n",
    "        # predicted words\n",
    "        pred_pos = [pos for pos, prob in tag_trigram_model[first_pos, second_pos].items() if prob == most_likely]\n",
    "    return pred_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NN']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_predict_next_pos('VBD', 'DT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
