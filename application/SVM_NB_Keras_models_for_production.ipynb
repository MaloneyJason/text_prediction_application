{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading transformer....\n",
      "loading transformer....done!\n",
      "loading MNB model (86% acuracy rate)....\n",
      "loading MNB model (86% acuracy rate)....done!\n",
      "peak memory: 479.59 MiB, increment: 393.19 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bing0\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.22.2.post1 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\bing0\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator MultinomialNB from version 0.22.2.post1 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "# %%time\n",
    "print('loading transformer....')\n",
    "# load the tfidf_vectorizer from disk\n",
    "filename = 'models/MNB_vect.sav'\n",
    "MNB_vect = pickle.load(open(filename, 'rb'))\n",
    "print('loading transformer....done!')\n",
    "\n",
    "print('loading MNB model (86% acuracy rate)....')\n",
    "# load the selector from disk 86% accuracy\n",
    "filename = 'models/MNB_model.sav' \n",
    "MNB_model = pickle.load(open(filename, 'rb'))\n",
    "print('loading MNB model (86% acuracy rate)....done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNB_predict_sentiment(text):\n",
    "    \"\"\"\n",
    "    expect: a string of text\n",
    "    modify: vectorize the string with 'tfidf_vectorizer' and 'selector' transformers\n",
    "    return: an int (1:'postive' ; 0:'negative')\n",
    "    \"\"\"\n",
    "    text_list=[]\n",
    "    text_list.append(text)\n",
    "    raw_dtm = MNB_vect.transform(text_list)\n",
    "    pred_class = int(MNB_model.predict(raw_dtm))    \n",
    "    return pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNB_predict_sentiment(\"\"\"This guy doesn't get comedy. Amy schumer is a great comedian, but her show is trying to shove \"funny\" down your throat so hard that it loses credibility. The same goes for the kroll show. Review is a fresh concept and Andy pulls off laughs without trying so hard you shudder from the douche chills. I felt like it had just enough painful awkwardness without going overboard and relying on it to carry the show.I honestly thought the show was gonna be dumb when i saw the previews but I laughed non stop through the whole first episode. If it was any other host I think the show would be a flop but Andy is a perfect fit and plays the part flawlessly IMO. I give it a MILLION STARS!!!!!!!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading transformer....\n",
      "loading transformer....done!\n",
      "loading SVM model (89% accuracy rate)....\n",
      "loading SVM model (89% accuracy rate)....done!\n",
      "peak memory: 796.86 MiB, increment: 410.19 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bing0\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator LinearSVC from version 0.22.2.post1 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "# %%time\n",
    "print('loading transformer....')\n",
    "# load the transformer from disk\n",
    "filename = 'models/SVM_vect.sav'\n",
    "SVM_vect = pickle.load(open(filename, 'rb'))\n",
    "print('loading transformer....done!')\n",
    "\n",
    "print('loading SVM model (89% accuracy rate)....')\n",
    "# load the model from disk  89%\n",
    "filename = 'models/SVM_model.sav'\n",
    "SVM_model = pickle.load(open(filename, 'rb'))\n",
    "print('loading SVM model (89% accuracy rate)....done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_predict_sentiment(text):\n",
    "    \"\"\"\n",
    "    expect: a string of text\n",
    "    modify: vectorize the string with 'tfidf_vectorizer' and 'selector' transformers\n",
    "    return: an int (1:'postive' ; 0:'negative')\n",
    "    \"\"\"\n",
    "    text_list=[]\n",
    "    text_list.append(text)\n",
    "    raw_dtm = SVM_vect.transform(text_list)\n",
    "    pred_class = int(SVM_model.predict(raw_dtm))    \n",
    "    return pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_predict_sentiment(\"\"\" This guy doesn't get comedy. Amy schumer is a great comedian, but her show is trying to shove \"funny\" down your throat so hard that it loses credibility. The same goes for the kroll show. Review is a fresh concept and Andy pulls off laughs without trying so hard you shudder from the douche chills. I felt like it had just enough painful awkwardness without going overboard and relying on it to carry the show.I honestly thought the show was gonna be dumb when i saw the previews but I laughed non stop through the whole first episode. If it was any other host I think the show would be a flop but Andy is a perfect fit and plays the part flawlessly IMO. I give it a MILLION STARS!!!!!!!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install PyYAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install anvil-uplink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 836.18 MiB, increment: 153.30 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "import keras # for sentitment model\n",
    "import tensorflow as tf  # to resolve the loading issue with different version of keras\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pickle # load transformers: 'tfidf_vectorizer' and 'selector'\n",
    "import dill # load language model\n",
    "from collections import defaultdict # to make language model work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading keras model (92% accuracy)...\n",
      "loading keras model (92% accuracy)...done!\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout (Dropout)            (None, 20000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                1280064   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "peak memory: 868.11 MiB, increment: 31.91 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "# load model 92%\n",
    "print('loading keras model (92% accuracy)...')\n",
    "model = tf.keras.models.load_model('models/kera_model_dropout_nn.h5')\n",
    "print('loading keras model (92% accuracy)...done!')\n",
    "\n",
    "# summarize model.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1578.82 MiB, increment: 710.71 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "# %%time\n",
    "# load the tfidf_vectorizer from disk\n",
    "filename = 'models/tfidf_vectorizer.sav'\n",
    "tfidf_vectorizer = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "# load the selector from disk\n",
    "filename = 'models/selector.sav'\n",
    "selector = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_predict_sentiment(text):\n",
    "    \"\"\"\n",
    "    expect: a string of text\n",
    "    modify: vectorize the string with 'tfidf_vectorizer' and 'selector' transformers\n",
    "    return: an int (1:'postive' ; 0:'negative')\n",
    "    \"\"\"\n",
    "    text_list=[]\n",
    "    text_list.append(text)\n",
    "    raw_dtm = tfidf_vectorizer.transform(text_list)\n",
    "    selected_dtm = selector.transform(raw_dtm).astype('float32')  \n",
    "    pred_class = int(model.predict_classes(selected_dtm.toarray()))    \n",
    "    return pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_predict_sentiment(\"\"\"comedy central is a vast wasteland of unsuccessful tv shows based around standup comics, but \"review\" is a refreshing winner. reviewing life itself (and playing the inanities of life to the extreme) is a simple concept, but it works, and really just churns out one ridiculously uncomfortable moment after another. but it's from that discomfort that the big laughs arise. andrew daly is perfect for the slightly hapless forrest macneil, and his exasperated wife and wryly awkward cohost are my favorite parts of the show.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading language model....\n",
      "loading language model....done!\n",
      "peak memory: 3205.05 MiB, increment: 1711.70 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "print('loading language model....')\n",
    "# %%time\n",
    "# Load models\n",
    "with open('models/bigram_model.p', 'rb') as file:\n",
    "   bigram_model = dill.load(file)\n",
    "with open('models/trigram_model.p', 'rb') as file:\n",
    "   trigram_model = dill.load(file)\n",
    "# with open('models/fourgram_model.p', 'rb') as file:\n",
    "#    fourgram_model = dill.load(file)\n",
    "# with open('models/fivegram_model.p', 'rb') as file:\n",
    "#    fivegram_model = dill.load(file)\n",
    "\n",
    "# define tokenizer to get words\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "print('loading language model....done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict the next word based on bigram model\n",
    "def bigram_predict_next_word(word):\n",
    "    '''\n",
    "    word: a list of token\n",
    "    '''\n",
    "    if len(bigram_model[word[0]]) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        prob_list = bigram_model[word[0]].values()\n",
    "        # find the max prob\n",
    "        most_likely = max(prob_list)\n",
    "        #print(most_likely)\n",
    "        # predicted words\n",
    "        pred_words = [word for word, prob in bigram_model[word[0]].items() if prob == most_likely]\n",
    "#         pred_word = random.choice(pred_words)\n",
    "    return pred_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict next word with trigram model\n",
    "def trigram_predict_next_word(words):\n",
    "    '''\n",
    "    words: a list of token\n",
    "    '''\n",
    "    if len(trigram_model[words[0], words[1]]) == 0:\n",
    "        last_word = words[-1]\n",
    "        return bigram_predict_next_word(last_word)\n",
    "    else:\n",
    "        # get probabilities of next word\n",
    "        prob_list = trigram_model[words[0], words[1]].values()\n",
    "        # find the max prob\n",
    "        most_likely = max(prob_list)\n",
    "        # predicted words\n",
    "        pred_words = [word for word, prob in trigram_model[words[0], words[1]].items() if prob == most_likely]\n",
    "    return pred_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourgram_predict_next_word(words):\n",
    "    '''\n",
    "    words: a list of token\n",
    "    '''\n",
    "    if len(fourgram_model[words[0], words[1], words[2]]) == 0:\n",
    "        last_two_words = words[-2:] \n",
    "        return trigram_predict_next_word(last_two_words)\n",
    "    else:\n",
    "        # get probabilities of next word\n",
    "        prob_list = fourgram_model[words[0], words[1], words[2]].values()\n",
    "        # find max prob\n",
    "        most_likely = max(prob_list)\n",
    "        # get the predicted word(s)\n",
    "        pred_words = [word for word, prob in fourgram_model[words[0], words[1], words[2]].items() if prob == most_likely]\n",
    "#         pred_word = random.choice(pred_words)\n",
    "    return pred_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict next word with 5-gram model\n",
    "def fivegram_predict_next_word(words):\n",
    "    '''\n",
    "    words: a list of token\n",
    "    '''\n",
    "    if len(fivegram_model[words[0], words[1], words[2], words[3]]) == 0:\n",
    "        last_three_words = words[-3:]\n",
    "        return fourgram_predict_next_word(last_three_words)\n",
    "    else:\n",
    "        # get probabilities of next word\n",
    "        prob_list = fivegram_model[words[0], words[1], words[2], words[3]].values()\n",
    "        # find max prob\n",
    "        most_likely = max(prob_list)\n",
    "        # predicted words\n",
    "        pred_words = [w for w, p in fivegram_model[words[0], words[1], words[2], words[3]].items() if p == most_likely]\n",
    "#         pred_word = random.choice(pred_words)\n",
    "    return pred_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_prediction(text):\n",
    "    \"\"\"\n",
    "    expect: a raw string of text\n",
    "    modify: tokenize the string and # check the length to decide to start with which model.\n",
    "                1. if the string has 4 tokens or above, take the last four tokens as input to fivegram_model\n",
    "                2. if the string has 3 tokens, take the list of tokens as input to fourgram_model\n",
    "                3. if the string has 2 tokens, take the list of tokens as input to trigram_model\n",
    "                4. if the string has 1 tokens, take the token as input to trigram_model\n",
    "    return: predicted word\n",
    "    \"\"\"\n",
    "    # tokenize the words\n",
    "    user_tokens = tokenizer.tokenize(text)\n",
    "    if len(user_tokens) >= 4:\n",
    "        try:\n",
    "            return fivegram_predict_next_word(user_tokens[-4:]) # take the last four tokens\n",
    "        except:    \n",
    "            return fivegram_predict_next_word(user_tokens) # take the last four tokens\n",
    "    elif len(user_tokens) == 3:\n",
    "        return fourgram_predict_next_word(user_tokens)\n",
    "    elif len(user_tokens) == 2:\n",
    "        return trigram_predict_next_word(user_tokens)\n",
    "    elif len(user_tokens) == 1:\n",
    "        return bigram_predict_next_word(user_tokens) \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for AWS EC2\n",
    "def make_prediction(text):\n",
    "    \"\"\"\n",
    "    expect: a raw string of text\n",
    "    modify: tokenize the string and # check the length to decide to start with which model.\n",
    "            1. if the string has 2 tokens or above, take the last four tokens as input to trigram_model\n",
    "            2. if the string has 1 token, take the list of tokens as input to bigram_model\n",
    "    return: predicted word\n",
    "    \"\"\"\n",
    "    # tokenize the words\n",
    "    user_tokens = tokenizer.tokenize(text)\n",
    "    if len(user_tokens) >= 2:\n",
    "        try:\n",
    "            return trigram_predict_next_word(user_tokens[-2:]) # take the last four tokens\n",
    "        except:    \n",
    "            return trigram_predict_next_word(user_tokens) # take the last four tokens\n",
    "    elif len(user_tokens) == 1:\n",
    "        return bigram_predict_next_word(user_tokens) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.3\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anvil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%memit\n",
    "# import anvil.server\n",
    "\n",
    "# anvil.server.connect(\"KDO33RPUPH27AGMVCZBAHU2U-PDHT24SNT663M2H7\")\n",
    "# print('final step is running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # @anvil.server.callable\n",
    "# # # Put the function that will be used in anvil client server (web)\n",
    "# # # This block of code should be running forever\n",
    "# # # The @anvil.server.callable decorator can only take one function\n",
    "\n",
    "# def app_prediction(text):\n",
    "#     \"\"\"\n",
    "#     expect: a string of text\n",
    "#     modify: send the string to two functions:\n",
    "#                 1. ngram_prediction\n",
    "#                 2. keras_predict_sentiment\n",
    "#     return: a tuple that contains a predicted word and a predicted class\n",
    "#     \"\"\"\n",
    "#     ## Sentiment Prediction\n",
    "#     pred_word = make_prediction(text)\n",
    "    \n",
    "#     ## Text Prediction\n",
    "#     pred_class = keras_predict_sentiment(text)\n",
    "        \n",
    "#     return pred_word, pred_class\n",
    "\n",
    "# # anvil.server.wait_forever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
